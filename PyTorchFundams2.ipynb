{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Variables and gradients \n",
    "### 2.1 Variables\n",
    "    - A variable wraps a Tensor\n",
    "    - Variables allow a Tensor to accumilate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a variable\n",
    "torch.ones(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  2\n",
      " 2  2\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 2  2\n",
      " 2  2\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Behaves similarly to Torch Tensors\n",
    "b = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print a + b\n",
    "print torch.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print a * b\n",
    "print (torch.mul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what exactly is require_grad?\n",
    "    - Allows for the calculations of gradients with respect to (w.r.t) the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i = 5(x_i + 1)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\therefore \\hspace{0.3cm} y_i \\mid_{x_i=1} = 5(1 + 1)^2 = 5(2)^2 = 5(4) = 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20\n",
       " 20\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 5 * (x + 1) ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Backward should be called only on a scalar(i.e. 1-element tensor) or with a gradient w.r.t the variable **\n",
    "    - So to get our gradient, we want to reduce y = Variable(torch.Tensor[20, 20]) to a single value... a scalar.\n",
    "    - To do this we've decided to get the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\therefore \\hspace{0.3cm} o = \\frac{1}{2}\\Sigma_{i}y_i = \\frac{\\Sigma_i y_i}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In code:\n",
    "o = (1.0/2) * torch.sum(y)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap $y$ equation: $y_i = 5(x_i + 1)^2$  \n",
    "  \n",
    "Recap $o$ equation: $o = \\frac{1}{2}\\Sigma_{i}y_i = \\frac{\\Sigma_i y_i}{2}$  \n",
    "\n",
    "So $o$ is the the mean of some hypothesis of $x$: \n",
    "\n",
    "$\\hspace{1cm}$ Substitute $y$ into $o$ equation:  \n",
    "\n",
    "$\\hspace{2cm} o = \\frac{1}{2}\\Sigma_{i}5(x_i + 1)^2$\n",
    "\n",
    "$\\hspace{2cm} \\therefore \\hspace{0.3cm} \\frac{\\partial o}{\\partial x_i} = \\frac{1}{2}[10(x_i + 1)]$\n",
    "\n",
    "$\\hspace{2cm} \\therefore \\hspace{0.3cm} \\frac{\\partial o}{\\partial x_i} \\mid_{x_i = 1} = \\frac{1}{2}[10(1 + 1)] = \\frac{10(2)}{2} = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       " 10\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o.backward gave us the gradient so now we can differentiate with do with respect to x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Variable\n",
    "    - Wraps around a tensor so that gradients can be accumilated.\n",
    "- Gradients\n",
    "    - The backprop function creates a computational graph.\n",
    "    - From the computational graph, we can use the chain rule to find the derivative of our hypothesis with respect to any of our parameters.\n",
    "    - To apply backpropagation, we need to reduce our hypothesis matrix into just a scalar. (One way is through calculating a mean.)\n",
    "    - We can then access any one of the gradient with respect to a parameter of our choosing through **```var.grad```** where we calculate the gradient of our **mean(hypothesis)** with respect to **```var```**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
